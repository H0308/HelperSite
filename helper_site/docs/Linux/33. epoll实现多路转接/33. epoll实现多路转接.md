# `epoll`实现多路转接

## 本篇介绍

在前面一节已经介绍了`select`和`poll`实现多路转接，但是`select`和`poll`都存在一些缺陷。而二者最大的缺陷就是都需要内核涉及到遍历操作。所以，为了尽可能减少内核的遍历，就需要用到`epoll`实现多路转接

## `epoll`接口介绍

要使用`epoll`实现多路转接，需要经过三个步骤：

1. 创建`epoll`模型
2. 设置需要关心的文件描述符和对应事件
3. 注册关心的文件描述符和事件

根据这三个步骤，分别使用三个不同的接口：

**创建`epoll`模型**

在Linux中，要使用`epoll`实现多路转接，需要使用`epoll_create`函数创建一个`epoll`模型，该函数声明如下：

```c
int epoll_create(int size);
```

尽管该函数存在一个参数，但是在2.6.8内核版本后，该参数已经被忽略，并且内核会使用一个默认值来代替，所以在使用时，该参数可以设置为任意值，一般设置为128或者256

!!! note

    需要注意，尽管`size`可以设置为任意值，但是必须要保证`size`大于0

该函数会返回创建的`epoll`模型对应的文件描述符

**设置需要关心的文件描述符和对应事件**

调用`epoll_create`函数创建好`epoll`模型后，需要使用`epoll_ctl`函数将需要关心的文件描述符和对应的事件添加到`epoll`模型中，该函数声明如下：

```c
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
```
该函数的第一个参数表示目标`epoll`模型对应的文件描述符，第二个参数表示操作类型，该参数有三种类型：

- `EPOLL_CTL_ADD`：将指定的文件描述符添加到`epoll`模型中
- `EPOLL_CTL_MOD`：修改指定文件描述符对应的事件
- `EPOLL_CTL_DEL`：将指定文件描述符从指定的`epoll`模型中删除

第三个参数表示需要关心的文件描述符，第四个参数表示需要关心的文件描述符对应的事件，该结构定义如下：

```c
union epoll_data 
{
    void     *ptr;
    int       fd;
    uint32_t  u32;
    uint64_t  u64;
};

typedef union epoll_data  epoll_data_t;

struct epoll_event 
{
    uint32_t      events;  /* Epoll events */
    epoll_data_t  data;    /* User data variable */
};
```

在`epoll_event`结构中，第一个字段表示文件描述符对应的事件，有下面的几种类型：

- `EPOLLIN`：表示关心文件描述符的读事件
- `EPOLLOUT`：表示关心文件描述符的写事件
- `EPOLLET`：表示以边沿触发的方式进行事件通知
- `EPOLLONESHOT`：表示文件描述符只能触发一次事件
- `EPOLLRDHUP`：表示文件描述符对应的连接被对方关闭
- `EPOLLPRI`：表示文件描述符对应的连接有紧急数据可读
- `EPOLLERR`：表示文件描述符对应的连接发生错误
- `EPOLLHUP`：表示文件描述符对应的连接被挂断

第二个字段表示文件描述符对应的用户数据，一般使用`fd`字段来表示文件描述符

**注册关心的文件描述符和事件**

在调用`epoll_ctl`函数将需要关心的文件描述符和对应的事件添加到`epoll`模型中后，就可以使用`epoll_wait`函数来等待事件的发生，该函数声明如下：

```c
int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout);
```

该函数的第一个参数表示目标`epoll`模型对应的文件描述符，最后一个参数的设置与`poll`一致，此处不再赘述。接着，第二个参数和第三个参数共同表示一个数组，与`poll`类似，`struct epoll_event *events`表示数组的第一个元素的地址，`maxevents`表示数组的元素个数。但是需要注意的是，这里的第二个参数并不是输入型参数，而是一个输出型参数，该数组中存储的是对应事件已经就绪的文件描述符，而因为事件可能不止一个，也有可能有很多个，所以需要`maxevents`来控制一次最多可以获取到的事件个数

该函数的返回值与`poll`一样，因为返回值大于0决定了具体就绪的文件描述符的个数，所以遍历就绪文件描述符数组`events`时需要用到该返回值

## 从接口层面对比`epoll`与`select`和`poll`

根据上面的接口介绍，可以看到`epoll`与`select`和`poll`最大的区别就在于接口的个数上，`epoll`只有三个接口，而`select`和`poll`根据前面的使用只有一个接口，而`epoll`三个接口中的后两个分别表示不同的功能：「用户需要内核关心的文件描述符和对应的事件」以及「内核告诉用户有哪些事件已经就绪」，所以从接口层面来看，`epoll`将这两个操作分离，使得操作变得更加清晰

## `epoll` 原理

虽然上面已经对`epoll`实现多路转接需要用到的接口进行了介绍，但是其中还涉及到一些更加细节的问题无法通过接口的声明来描述，所以除了需要知道`epoll`需要用的到的接口外，还需要了解`epoll`的实现原理

<!-- 下面的内容可能需要迁移，迁移起始 -->

在前面不论是编写UDP服务器和编写TCP服务器第一步都需要创建套接字，而这个套接字本质还是一个文件描述符，那么文件描述符是如何与套接字产生关联的？

为了解决这个问题，首先需要看`struct file`结构，该结构定义如下：

```c
struct file {
	union {
		struct list_head	fu_list;
		struct rcu_head 	fu_rcuhead;
	} f_u;
	struct dentry		*f_dentry;
	struct vfsmount         *f_vfsmnt;
	const struct file_operations	*f_op;
	atomic_t		f_count;
	unsigned int 		f_flags;
	mode_t			f_mode;
	loff_t			f_pos;
	struct fown_struct	f_owner;
	unsigned int		f_uid, f_gid;
	struct file_ra_state	f_ra;

	unsigned long		f_version;
	void			*f_security;

	/* needed for tty driver, and maybe others */
	void			*private_data;

#ifdef CONFIG_EPOLL
	/* Used by fs/eventpoll.c to link all the hooks to this file */
	struct list_head	f_ep_links;
	spinlock_t		f_ep_lock;
#endif /* #ifdef CONFIG_EPOLL */
	struct address_space	*f_mapping;
};
```

在该结构中有一个指针`private_data`，这个指针的类型是`void *`，所以可以指向任意类型的数据。在网络部分，一旦创建了套接字，那么就会创建一个`struct socket`结构，该结构定义如下：

```c
struct socket {
	socket_state		state;
	unsigned long		flags;
	const struct proto_ops	*ops;
	struct fasync_struct	*fasync_list;
	struct file		*file;
	struct sock		*sk;
	wait_queue_head_t	wait;
	short			type;
};
```

而要实现文件描述符与套接字的关联，就需要将`struct socket`结构体对象的地址赋值给`struct file`结构的`private_data`指针

另外，在`struct socket`结构中，存在着一个成员`struct sock *sk`，这个成员表示具体的某一个套接字类型，既可以是UDP套接字也可以是TCP套接字，该类型的定义如下：

```c
struct sock 
{
	struct sock_common	__sk_common;
#define sk_family		__sk_common.skc_family
#define sk_state		__sk_common.skc_state
#define sk_reuse		__sk_common.skc_reuse
#define sk_bound_dev_if		__sk_common.skc_bound_dev_if
#define sk_node			__sk_common.skc_node
#define sk_bind_node		__sk_common.skc_bind_node
#define sk_refcnt		__sk_common.skc_refcnt
#define sk_hash			__sk_common.skc_hash
#define sk_prot			__sk_common.skc_prot
	unsigned char		sk_shutdown : 2,
				sk_no_check : 2,
				sk_userlocks : 4;
	unsigned char		sk_protocol;
	unsigned short		sk_type;
	int			sk_rcvbuf;
	socket_lock_t		sk_lock;
	wait_queue_head_t	*sk_sleep;
	struct dst_entry	*sk_dst_cache;
	struct xfrm_policy	*sk_policy[2];
	rwlock_t		sk_dst_lock;
	atomic_t		sk_rmem_alloc;
	atomic_t		sk_wmem_alloc;
	atomic_t		sk_omem_alloc;
	struct sk_buff_head	sk_receive_queue;
	struct sk_buff_head	sk_write_queue;
	struct sk_buff_head	sk_async_wait_queue;
	int			sk_wmem_queued;
	int			sk_forward_alloc;
	gfp_t			sk_allocation;
	int			sk_sndbuf;
	int			sk_route_caps;
	int			sk_gso_type;
	int			sk_rcvlowat;
	unsigned long 		sk_flags;
	unsigned long	        sk_lingertime;

	struct {
		struct sk_buff *head;
		struct sk_buff *tail;
	} sk_backlog;
	struct sk_buff_head	sk_error_queue;
	struct proto		*sk_prot_creator;
	rwlock_t		sk_callback_lock;
	int			sk_err,
				sk_err_soft;
	unsigned short		sk_ack_backlog;
	unsigned short		sk_max_ack_backlog;
	__u32			sk_priority;
	struct ucred		sk_peercred;
	long			sk_rcvtimeo;
	long			sk_sndtimeo;
	struct sk_filter      	*sk_filter;
	void			*sk_protinfo;
	struct timer_list	sk_timer;
	struct timeval		sk_stamp;
	struct socket		*sk_socket;
	void			*sk_user_data;
	struct page		*sk_sndmsg_page;
	struct sk_buff		*sk_send_head;
	__u32			sk_sndmsg_off;
	int			sk_write_pending;
	void			*sk_security;
	void			(*sk_state_change)(struct sock *sk);
	void			(*sk_data_ready)(struct sock *sk, int bytes);
	void			(*sk_write_space)(struct sock *sk);
	void			(*sk_error_report)(struct sock *sk);
  	int			(*sk_backlog_rcv)(struct sock *sk,
						  struct sk_buff *skb);  
	void                    (*sk_destruct)(struct sock *sk);
};
```

在这个结构中就存在着UDP和TCP需要用到的缓冲区成员

如果看得到UDP套接字和TCP套接字的相关结构定义：

=== "UDP套接字"

    ```c
    struct udp_sock 
    {
        /* inet_sock has to be the first member */
        struct inet_sock inet;
        // ...
    };
    ```

=== "TCP套接字"

    ```c
    struct inet_connection_sock 
    {
        /* inet_sock has to be the first member! */
        struct inet_sock	  icsk_inet;
        // ...
    };
    
    struct tcp_sock 
    {
        /* inet_connection_sock has to be the first member of tcp_sock */
        struct inet_connection_sock	inet_conn;
        // ...
    }
    ```

可以发现，在`tcp_sock`结构和`udp_sock`中都存在着一个成员`struct inet_sock inet`，在这个类型中：

```c
struct inet_sock {
	/* sk and pinet6 has to be the first two members of inet_sock */
	struct sock		sk;
    // ...
};
```

第一个成员就是`struct sock`类型，所以可以得出下图：

<img src="33. epoll实现多路转接.assets/image-20250408162710712.png">

这就是在网络套接字部分实现的继承和多态，这一点在前面[操作系统管理System V标准中三种资源的方式](https://www.help-doc.top/Linux/17.%20Linux%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/4.%20%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E7%AE%A1%E7%90%86System%20V%E6%A0%87%E5%87%86%E4%B8%AD%E4%B8%89%E7%A7%8D%E8%B5%84%E6%BA%90%E7%9A%84%E6%96%B9%E5%BC%8F/4.%20%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E7%AE%A1%E7%90%86System%20V%E6%A0%87%E5%87%86%E4%B8%AD%E4%B8%89%E7%A7%8D%E8%B5%84%E6%BA%90%E7%9A%84%E6%96%B9%E5%BC%8F.html#system-v)也有类似的结构形式

<!-- 迁移结束 -->

知道了文件描述符如何与套接字进行关联后，接着看`epoll`实现多路转接的原理：

首先用户调用`epoll_create`函数创建`epoll`模型，该函数会在内核中创建一个`struct eventpoll`类型的对象，该对象中包含一个`struct rb_root`类型的对象，该对象是一个红黑树，该红黑树的节点类型是`struct epitem`。这两个结构定义分别如下：

=== "`struct eventpoll`"

    ```c
    struct eventpoll 
    {
    	/* Protect the this structure access */
    	rwlock_t lock;
    
    	/*
    	* This semaphore is used to ensure that files are not removed
    	* while epoll is using them. This is read-held during the event
    	* collection loop and it is write-held during the file cleanup
    	* path, the epoll file exit code and the ctl operations.
    	*/
    	struct rw_semaphore sem;
    
    	/* Wait queue used by sys_epoll_wait() */
    	wait_queue_head_t wq;
    
    	/* Wait queue used by file->poll() */
    	wait_queue_head_t poll_wait;
    
    	/* List of ready file descriptors */
    	struct list_head rdllist;
    
    	/* RB-Tree root used to store monitored fd structs */
    	struct rb_root rbr;
    };
    ```

=== "`struct epitem`"

    ```c
    struct epitem {
    	/* RB-Tree node used to link this structure to the eventpoll rb-tree */
    	struct rb_node rbn;
    
    	/* List header used to link this structure to the eventpoll ready list */
    	struct list_head rdllink;
    
    	/* The file descriptor information this item refers to */
    	struct epoll_filefd ffd;
    
    	/* Number of active wait queue attached to poll operations */
    	int nwait;
    
    	/* List containing poll wait queues */
    	struct list_head pwqlist;
    
    	/* The "container" of this item */
    	struct eventpoll *ep;
    
    	/* The structure that describe the interested events and the source fd */
    	struct epoll_event event;
    
    	/*
    	* Used to keep track of the usage count of the structure. This avoids
    	* that the structure will desappear from underneath our processing.
    	*/
    	atomic_t usecnt;
    
    	/* List header used to link this item to the "struct file" items list */
    	struct list_head fllink;
    
    	/* List header used to link the item to the transfer list */
    	struct list_head txlink;
    
    	/*
    	* This is used during the collection/transfer of events to userspace
    	* to pin items empty events set.
    	*/
    	unsigned int revents;
    };
    
    /* Wrapper struct used by poll queueing */
    struct ep_pqueue {
    	poll_table pt;
    	struct epitem *epi;
    };
    ```

在`struct eventpoll`结构中，除了有红黑树结构以外，还存在着`struct list_head rdllist;`，这个链表中存储的就是就绪的文件描述符，而在`struct epitem`结构中，`struct epoll_filefd ffd;`表示的就是具体的文件描述符，而`struct epoll_event event;`表示的就是文件描述符对应的事件

结合前面的三个接口看具体的原理就是首先调用`epoll_create`函数创建`epoll`模型，接着调用`epoll_ctl`函数将需要关心的文件描述符和对应的事件添加到`epoll`模型中，然后调用`epoll_wait`函数等待事件的发生，当事件发生后，`epoll`模型会将对应的文件描述符添加到`rdllist`链表中，然后`epoll_wait`函数会返回，用户就可以遍历`rdllist`链表来获取就绪的文件描述符

但是，这里还涉及到一个问题，就是`epoll_wait`函数是如何知道事件已经就绪的？这里实际上就是利用到了**底层回调机制**。所谓底层回调机制，可以理解为内核先准备一个函数指针，但是这个指针一开始指向为空，当存在一个事件就绪时，该指针就会指向一个具体的函数，接着，内核会调用该函数执行其中的函数体，在此处可以简单理解为函数体就是将挂在红黑树上的文件描述符节点添加到`rdllist`链表中

整个过程示意图如下：

<img src="33. epoll实现多路转接.assets/image-20250408164946326.png">

分析完上面的基本原理，下面思考几个问题：

1. `epoll`模型中使用到的红黑树结构相当于`select`和`poll`模型中的什么结构？
2. 红黑树本质就是`key-value`模型，那么什么元素作为`key`？
3. 为什么`epoll_create`函数返回的是一个文件描述符？
4. `epoll`模型为什么比`select`和`poll`模型更加高效？

基于上面的问题，下面给出答案：

1. `epoll`模型中使用到的红黑树结构相当于`select`和`poll`模型中的辅助数组，因为本质都是保存着用户需要关心的文件描述符
2. 实际上`key`就是文件描述符，通过文件你描述符可以快速找到具体的一个红黑树节点，查找效率高
3. 在Linux中一切皆文件，而在`struct file`中存在一个`private_data`指针，<a href="javascript:;" class="custom-tooltip" data-title='根据Linux内核中的描述：This structure is stored inside the "private_data" member of the file structure'>这个指针在`epoll`模型中指向`struct eventpoll`结构</a>
4. 首先，从内核查找用户需要关系的文件描述符时，`epoll`模型只需要查找红黑树，而`select`和`poll`模型需要遍历整个数组，这个时间消耗上比纯数组要低。其次，当有事件就绪时，对应的红黑树节点会被直接添加到`rdlist`中，这就可以避免了`select`和`poll`模型中需要遍历整个数组的过程，而用户在调用`epoll_wait`函数时获取到的`struct epoll_event *events`数组一定是包含着就绪的文件描述符的，此时用户就需要关心这个数组中的数据即可。所以，`epoll`模型比`select`和`poll`模型更加高效

## `epoll`实现多路转接

上面已经基本介绍了`epoll`的接口和实现原理，下面结合上面的介绍对前面通过`poll`实现的TCP服务器进行修改：

首先是初始化部分，因为需要创建`epoll`模型，所以可以考虑在`epollServer`的构造函数中进行创建。因为`epoll_create`函数会返回一个文件描述符表示`epoll`模型，而且在后面的接口中也会使用到这个文件描述符，且该文件描述符只需要1份，所以考虑创建一个成员变量用于保存该文件描述符：

```cpp
class EpollServer
{
public:
	EpollServer(uint16_t port = default_port)
		: bs_(std::make_shared<TcpSocket>()), isRunning_(false), efd_(-1)
	{
		bs_->initSocket();
		// 创建epoll模型
		efd_ = epoll_create(256);
	}

private:
	// ...
	int efd_;
};
```

接着，因为服务器需要监听，所以需要在构造函数中将监听套接字和对应的写事件添加到`epoll`模型中：

```cpp
EpollServer(uint16_t port = default_port)
	: bs_(std::make_shared<TcpSocket>(port)), isRunning_(false), efd_(-1)
{
	// ...
	// 关心listen_socketfd
	struct epoll_event ee;
	// 关心读事件
	ee.events = EPOLLIN;
	ee.data.fd = bs_->getListenSocketFd();
	int ret = epoll_ctl(efd_, EPOLL_CTL_ADD, bs_->getListenSocketFd(), &ee);
}
```

启动服务器时，需要`epoll`模型进行等待，所以在`startServer`函数中将原来的`poll`接口替换为`epoll_wait`接口，另外，本次考虑使用间隔1秒的方式进行等待，所以还需要设置`epoll_wait`函数的超时时间：

```cpp
void startServer()
{
	isRunning_ = true;
	while (isRunning_)
	{
		// 不再需要在循环中多次设置监听套接字到读文件描述符集中
		// 等待1秒
		const int timeout = 1000;
		// 只关心读事件的文件描述符集
		int n = epoll_wait(efd_, fd_array_->data(), g_fd_array_num, timeout);
		if (n > 0)
		{
			// 监听套接字文件描述符准备完毕
			handler(n);
		}
		else if (n == 0)
		{
			LOG(LogLevel::INFO) << "没有客户端连接";
			sleep(1);
		}
	}
	isRunning_ = false;
}
```

此时，一旦监听套接字就绪，就说明有对应的客户端进行了连接，此时调用`toAccept`函数获取到对应的`ac_socketfd`，并将其添加到`epoll`模型中，但是`epoll`模型中可能不只有一个文件描述符和对应的事件，所以需要遍历`epoll_wait`接口返回的数组，这里因为内核添加内容是按照数组从0下标开始填充，所以可以按照正常遍历数组的方式进行，判断当前是否是监听套接字，如果是监听套接字就调用`toAccept`函数进行处理，否则就调用`recvData`函数进行处理：

```cpp
void handler(int n)
{
	for (int i = 0; i < n; i++)
	{
		// 保证一定是就绪事件
		// 判断当前是否是监听套接字
		if((*fd_array_)[i].data.fd == bs_->getListenSocketFd())
		{
			// 获取链接
			SockAddrIn client;
			int ac_socketfd = bs_->toAccept(std::addressof(client));

			if(ac_socketfd > 0)
			{
				// 添加到epoll模型中
				struct epoll_event ev;
				ev.events = EPOLLIN;
				ev.data.fd = ac_socketfd;
				int ret = epoll_ctl(efd_, EPOLL_CTL_ADD, ac_socketfd, &ev);
				if (ret < 0)
				{
					LOG(LogLevel::WARNING) << "添加事件失败";
					exit(static_cast<int>(ErrorNumber::Epoll_Ctl_Fail));
				}
			}
		}
		else 
		{
			// 不是监听套接字
			std::string buffer;
			ssize_t ret = bs_->recvData(buffer, (*fd_array_)[i].data.fd);
			if(ret > 0)
			{
				LOG(LogLevel::INFO) << "客户端发送：" << buffer;
			}
			else if(ret == 0)
			{
				// 恢复操作
				// 从红黑树中移除指定文件描述符，此时不再需要关心事件
				int ret = epoll_ctl(efd_, EPOLL_CTL_DEL, (*fd_array_)[i].data.fd, nullptr);
				if (ret < 0)
				{
					LOG(LogLevel::WARNING) << "移除事件失败";
					exit(static_cast<int>(ErrorNumber::Epoll_Ctl_Fail));
				}
			}
			else
			{
				int ret = epoll_ctl(efd_, EPOLL_CTL_DEL, (*fd_array_)[i].data.fd, nullptr);
				if (ret < 0)
				{
					LOG(LogLevel::WARNING) << "移除事件失败";
					exit(static_cast<int>(ErrorNumber::Epoll_Ctl_Fail));
				}
			}
		}
	}
}
```

与`select`和`poll`一样，此时不可以直接读取，虽然`toAccept`此时不会阻塞，但是`toRead`函数会阻塞，所以需要现将`ac_socketfd`添加到`epoll`模型中，再下一次遍历时一旦是`ac_socketfd`就绪，就可以调用`recvData`函数进行数据读取

直接编译运行上面的代码会发现，连接客户端没有问题和接收客户端发送的消息时没有问题，但是在客户端退出时会提示：

```
[2025-04-09 11-18-28] [WARNING] [11674] [epollServer.hpp] [103] - 移除事件失败
```

出现这个问题的原因是当`recvData`中的`recv`函数返回0时，会将对应的文件描述符关闭，如下面的逻辑：

```cpp
ssize_t recvData(std::string &out_data, int ac_socketfd) override
{
	char buffer[4096] = {0};
	ssize_t ret = recv(ac_socketfd, buffer, sizeof(buffer), 0);
	if (ret > 0)
		out_data = buffer;
	else
		close(ac_socketfd);

	return ret;
}
```

一旦文件描述符被关闭，那么此时就会造成对应的文件描述符变成无效的文件描述符，而`epoll_wait`函数如果要操作指定的文件描述符必须要保证该文件描述符是有效的，所以此时就会出现上面的错误提示，所以考虑移除`recvData`函数中的`close`函数，此时的代码如下：

```cpp
// 接收数据
ssize_t recvData(std::string &out_data, int ac_socketfd) override
{
	char buffer[4096] = {0};
	ssize_t ret = recv(ac_socketfd, buffer, sizeof(buffer), 0);
	if (ret > 0)
		out_data = buffer;

	return ret;
}
```

接着，在调用`epoll_ctl`函数之后关闭文件描述符：

```cpp
void handler(int n)
{
	for (int i = 0; i < n; i++)
	{
		// 保证一定是就绪事件
		// 判断当前是否是监听套接字
		// ...
		else 
		{
			// 不是监听套接字
			std::string buffer;
			ssize_t ret = bs_->recvData(buffer, (*fd_array_)[i].data.fd);
			// ...
			else if(ret == 0)
			{
				// ...
				// 在移除之后关闭文件描述符
				close((*fd_array_)[i].data.fd);
				if (ret < 0)
				{
					LOG(LogLevel::WARNING) << "移除事件失败";
					exit(static_cast<int>(ErrorNumber::Epoll_Ctl_Fail));
				}
			}
			else
			{
				// ...
				// 在移除之后关闭文件描述符
				close((*fd_array_)[i].data.fd);
				if (ret < 0)
				{
					LOG(LogLevel::WARNING) << "移除事件失败";
					exit(static_cast<int>(ErrorNumber::Epoll_Ctl_Fail));
				}
			}
		}
	}
}
```

运行上面的代码可以发现与`select`和`poll`一样实现了多路转接

## 水平触发和边缘触发

在`epoll`模型中，有两种触发方式：

1. 水平触发（LT，Level Trigger）：当文件描述符对应的事件发生时，会一直通知上层，直到上层将事件处理完毕
2. 边缘触发（ET，Edge Trigger）：当文件描述符对应的事件发生时，如果没有数据增多时，只会通知上层一次，这就导致了如果上层没有及时处理事件，那么后续的事件就会丢失，所以边缘出发会强制用户一次性读取完所有的数据

实现水平触发和边缘触发的逻辑可以理解为：

- **水平触发(LT)**：当有事件就绪时，该事件对应的红黑树节点会被添加到就绪队列`rdlist`中，只要继续队列中存在节点就会一直通知上层，直到上层将事件处理完毕

- **边缘触发(ET)**：当有事件就绪时，内核只会在刚就绪时通知一次，除非后续该文件描述符上关心的事件有数据增加。比如从无数据到有数据时触发一次`EPOLLIN`，之后即使缓冲区还有数据也不会再次通知

默认情况下，`epoll`使用的是水平触发模式。但是实际上，边缘触发模式更加高效，但是因为没有多次通知，所以要确保数据被完全读取完毕，就需要循环调用读取函数知道读到返回值为0为止，但是如果返回值为0，那么根据前面的经验，读取函数就会被阻塞，此时尽管`epoll`没有阻塞，但是读取函数一旦阻塞，服务器还是会卡住针对这个问题，解决方案就是**将读取文件描述符设置为非阻塞**。那么为什么边缘触发模式更高效？实际上，因为需要上层一次性把所有数据读取完毕，那么只要开始读取对应的接收缓冲区就可以保证越来越大，下一次服务端向客户端返回的窗口大小也会变大，从而提高了IO吞吐量，所以边缘触发模式更加高效

??? info "IO吞吐量"

	IO吞吐量（Input/Output Throughput）是。IO吞吐量指单位时间内系统能处理的数据量，通常以MB/s或GB/s为单位，常用于衡量系统IO性能的关键指标，特别是在高并发网络编程中。在网络编程中，它反映了服务器处理网络数据的能力
	
	在Linux下可以使用`iftop`命令查看IO吞吐量，但是首先需要安装`iftop`工具：
	
	```bash
	sudo apt install iftop
	```
	
	再使用下面的命令查看IO吞吐量：
	
	```bash
	sudo iftop -i eth0 -n -P
	```

## 基于边缘触发模式的`epoll`实现基本TCP服务器结构

### 基本思路

本次为了后面实现方便，首先对使用`epoll`实现多路转接的接口进行封装，接着，为了保证低耦合度，考虑将每一个客户端与服务端的连接设计为一个连接结构`Connection`的对象，这样可以保证在`EpollServer`看来，只有一个一个的连接对象而不是各种文件描述符。但是，除了有用于客户端和服务端进行数据通信的文件描述符外，还有监听套接字对应的文件描述符，而对于监听套接字来说，实际上其只关心读时间，所以可以考虑将监听套接字对应的文件描述符和普通的文件描述符看做一类连接结构对象，只是需要实现的方法不同

上面的问题解决了底层`EpollServer`和上层连接之间的关系，但是上层连接有两种情况：

1. 文件描述符对应的是监听套接字，执行的行为是建立客户端与服务端的连接
2. 文件描述符对应的是普通的文件描述符，执行的行为是与客户端进行数据通信

所以对于这一点，可以考虑设计两个类，一个类是`Listener`，表示的是监听套接字和其对应接口的封装，另一个类是`IOService`，表示的是普通的文件描述符和其对应接口的封装

### 实现`Connection`类基本结构

首先是`Connection`类，该类需要管理每一次的连接，所以需要有一个成员变量用于保存对应的文件描述符。另外，在前面不论是`select`与`poll`还是`epoll`实现的TCP服务器都存在着一个问题：读操作不能保证读取到的是完整的数据。因为前面的实现中缓冲区都是一个临时变量，一旦离开了当前读取的过程就会被销毁，为了解决读取到完整的数据，就必须对上一次读取的数据进行缓存，再次读取时将该数据与上一次的数据进行拼接直到有完整的数据，所以考虑在`Connection`类中还需要添加`in_buffer_`成员，同样的再提供一个`out_buffer_`成员。接着，因为底层的`EpollServer`管理的是连接结构对象，所以为了可以看到客户端的信息，还需要提供一个用于保存客户端信息结构的成员变量，这个类型即为前面封装的`SockAddrIn`类，所以当前`Connection`类的结构如下：

```cpp
class Connection
{
private:
	int fd_; // 当前套接字
	std::string in_buffer_; // 读取缓冲区
	std::string out_buffer_; // 写入缓冲区
	SockAddrIn client_; // 客户端信息
};
```

除了上面的信息外，为了保证当前`Connection`类既可以表示普通的文件描述符，还可以表示监听套接字，这里可以考虑将`Connection`作为基类，提供三个纯虚函数由子类进行实现，分别表示读、写和异常，如下：

```cpp
class Connection
{
public:
	// ...

	// 读、写和异常纯虚函数
	virtual void recvData() = 0;
	virtual void sendData() = 0;
	virtual void handleException() = 0;

	// ...
};
```

接着，为了保证子类可以访问到`Connection`类的成员变量，这里可以将`Connection`类的成员变量设置为`protected`，这样子类就可以直接访问到父类的成员变量，如下：

```cpp
class Connection
{
// ...
// private:
protected:
// ...
};
```

因为`EpollServer`类会管理每一个`Connection`对象，而`EpollServer`类会对每一个文件描述符进行关心，但是具体关心哪种事件当前在`Connection`类中并没有体现，所以还需要在`Connection`添加一个成员变量用于表示当前`Connection`关心的事件，类型为`uint32_t`，如下：

```cpp
class Connection
{
private:
	// ...

	uint32_t events_; // 事件类型
};
```

同样的，可以添加一个成员变量`revents`表示就绪的事件：

```cpp
class Connection
{
private:
	// ...

	uint32_t revents_; // 就绪事件类型
};
```

接下来需要对一些成员变量进行初始化，本次考虑在`Connection`类的构造函数中进行初始化操作：

```cpp
Connection()
	:fd_(-1), events_(0), revents_(0)
{}
```

接着，提供一些设置函数，如下：

=== "设置`client_`"

	```cpp
	// 设置client
	void setClient(SockAddrIn&& client)
	{
		client_ = client;
	}
	```

=== "设置`events_`"

	```cpp
	// 设置事件
	void setEvent(uint32_t events)
	{
		events_ = events;
	}
	```

=== "设置`revents_`"

	```cpp
	// 设置就绪事件
	void setRevents(uint32_t revents)
	{
		revents_ = revents;
	}
	```

=== "设置`fd_`"

	```cpp
	// 设置文件描述符
	void setFd(int fd)
	{
		fd_ = fd;
	}
	```

最后，提供一些获取函数，如下：

=== "获取客户端信息"

	```cpp
	// 获取客户端信息
	SockAddrIn getClientInfo()
	{
		return client_;
	}
	```

=== "获取文件描述符"

	```cpp
	// 获取文件描述符
	int getFd()
	{
		return fd_;
	}
	```

=== "获取事件"

	```cpp
	// 获取事件类型
	uint32_t getEvents()
	{
		return events_;
	}
	```

=== "获取就绪事件"

	```cpp
	// 获取就绪事件类型
	uint32_t getRevents()
	{
		return revents_;
	}
	```

### 实现`EpollServer`类基本结构

接着是`EpollServer`类，该类需要管理每一个`Connection`对象，因为`EpollServer`类需要对具体的描述符进行关心，而根据`Connection`类的设计：包含需要关心的事件，所以可以考虑在`EpollServer`类中创建一张哈希表存储文件描述符和`Connection`类对象的映射关系，本次考虑实现文件描述符和`Connection`类对象指针进行映射的方式如下：

```cpp
using conn_t = std::shared_ptr<Connection>;

class EpollServer
{
private:
	std::unordered_map<int, conn_t> connections_; // 文件描述符与Connection类对象指针的映射关系
};
```

接着，因为`EpollServer`类会接收`epoll_wait`返回的就绪事件数组，所以可以考虑在`EpollServer`类中创建一个数组用于存储，这里使用定长数组，在构造时初始化对应的指针，如下：

```cpp
class EpollServer
{
public:
	EpollServer()
		:revents_arr_(std::make_shared<std::array<struct epoll_event, g_default_array_num>>())
	{

	}

private:
	// ...
	std::shared_ptr<std::array<struct epoll_event, g_default_array_num>> revents_arr_; // 就绪事件数组
};
```

### 实现`epoll`接口封装类`Epoll`基本结构

因为`epoll`的设置接口和等待接口都会使用到`epoll`模型对应的文件描述符，所以考虑在`Epoll`类中创建一个成员变量存储该文件描述符，接着，既然是封装接口，就没有必要单独提供一个创建`epoll`模型的接口，所以可以考虑在`Epoll`类的构造函数中创建`epoll`模型，如下：

```cpp
class Epoll
{
public:
	Epoll()
		:epfd_(-1)
	{
		epfd_ = epoll_create(256);
		if(epfd_ < 0)
		{
			LOG(LogLevel::ERROR) << "Epoll模型创建失败";
			exit(static_cast<int>(ErrorNumber::Epoll_Create_Fail));
		}

		LOG(LogLevel::INFO) << "Epoll模型创建成功";
	}
	
private:
	int epfd_;
};
```

接着在`EpollServer`中添加一个成员变量指针用于表示`Epoll`类对象，这样在`EpollServer`类中就可以调用封装后的接口。在构造函数初始化列表中对该指针进行初始化：

```cpp
class EpollServer
{
public:
	EpollServer()
		// ...
		,ep_(std::make_shared<Epoll>())
	{

	}

private:
	// ...
	std::shared_ptr<Epoll> ep_; // 封装的Epoll类
};
```

### 设计`Epoll`类和`EpollServer`类

在下面进行设计之前先回顾之前的思路：

在本次实现中，`Epoll`接口封装类是最底层的类，而`EpollServer`类是`Epoll`接口封装类的上层类，这个服务器用于处理IO，而网络服务本质都是IO，所以网络服务相关的类（客户端与服务端建立连接和客户端与服务端进行数据通信）都在`EpollServer`类的上层，但是为了统一`EpollServer`视角，利用到了`Connection`类的中间层

首先既然要将文件描述符添加到`epoll`模型中，那么首先需要的就是在`Epoll`类中提供添加文件描述符到`epoll`模型的接口，如下：

```cpp
void epollCtl(int fd, uint32_t events)
{
	struct epoll_event ev;
	ev.events = events;
	ev.data.fd = fd;
	int ret = epoll_ctl(epfd_, EPOLL_CTL_ADD, fd, &ev);
	if(ret < 0)
	{
		LOG(LogLevel::WARNING) << "添加文件描述符和事件失败";
		exit(static_cast<int>(ErrorNumber::Epoll_Ctl_Fail));
	}

	LOG(LogLevel::INFO) << "添加文件描述符和事件成功";
}
```

接着，在`EpollServer`类提供一个将`Connection`类对象添加到`epoll`模型中的接口，但是参数是`Connection`类对象的指针，因为`EpollServer`类中管理的是`Connection`类对象的指针，如下：

```cpp
// 添加文件描述符和关心的事件
void insertFdAndEvents(conn_t con)
{
	auto pos = connections_.find(con->getFd());
	if(pos != connections_.end())
	{
		LOG(LogLevel::WARNING) << "插入失败，指定描述符已存在";
		exit(static_cast<int>(ErrorNumber::Epoll_Ctl_Fail));
	}

	// 将指定的文件描述符和对应的连接对象建立映射关系
	connections_.insert({con->getFd(), con});

	// 添加到Epoll模型中
	ep_->epollCtl(con->getFd(), con->getEvents());
}
```

因为`EpollServer`类需要等待每一个文件描述符就绪，所以需要`Epoll`提供一个等待接口，如下：

```cpp
// 等待就绪事件
int epollWait(struct epoll_event* ep_arr, int maxSize, int timeout)
{
	int num = epoll_wait(epfd_, ep_arr, maxSize, timeout);

	if(num < 0)
	{
		LOG(LogLevel::WARNING) << "Epoll等待错误";
		exit(static_cast<int>(ErrorNumber::Epoll_Wait_Fail));
	}

	LOG(LogLevel::INFO) << "Epoll开始等待";

	return num;
}
```

接着，在`EpollServer`类中实现服务器启动的函数，对于`EpollServer`来说，其主要任务就是等待文件描述符就绪，为了后续方便添加新功能，可以考虑将等待行为抽取到一个函数`loopOnce`中，而启动服务器函数就是启动服务器并持续执行`loopOnce`函数。为了标识服务器已经启动，可以使用一个成员变量`isRunning_`，在构造函数中初始化为`false`，如下：

```cpp
class EpollServer
{
public:
	EpollServer()
		// ...
		,isRunning_(false)
	{

	}

	// ...

	// 单次循环
	void loopOnce()
	{
		
	}

	// 启动服务器
	void startServer()
	{
		isRunning_ = true;
		while (isRunning_)
		{
			loopOnce();
		}
		
		isRunning_ = false;
	}

	~EpollServer()
	{}

private:
	// ...
	bool isRunning_; // 服务器运行标识
};
```

同样，可以提供函数用于停止服务器：

```cpp
// 停止服务器
void stopServer()
{
	isRunning_ = false;
}
```

接着，实现单次循环函数`loopOnce`，该函数就是等待已经存在于`epoll`模型中的文件描述符，并对具体的就绪事件进行处理。参考思路：单次循环中需要调用`Epoll`类中的`epollWait`函数，该函数会返回已经就绪事件的个数和数组，遍历数组获取到每一个继续的文件描述符和对应的事件，如果返回的事件是错误事件，为了处理方便，将该返回事件修改为读写事件就绪`EPOLLIN | EPOLLOUT`，交给上层的读写函数处理，这一点具体作用在后面会提及，此处不过多解释。接着就是正常情况，即要么是读事件就绪，要么是写事件继续，要么就是二者依次就绪，所以需要两个判断分别处理，但是此处不能只通过判断返回的就绪事件类型是否是或者写就决定执行某一种分支，而是还要判断对应的文件描述符是否存在。在执行就绪事件对应的逻辑分支中，因为哈希表的`value`是`Connection`对象指针类型，只需要调用父类的方法即可，如果是监听套接字，那么就会执行创建连接，否则就是正常的读写。根据这个思路，需要额外实现一个函数判断当前文件描述符是否存在于哈希表中：

```cpp
// 判断文件描述符是否存在
bool checkFdIsInConnections(int fd)
{
	return connections_.find(fd) != connections_.end();
}
```

接着实现`loopOnce`函数：

```cpp
// 单次循环
void loopOnce()
{
	int timeout = 1000;
	int num = ep_->epollWait(revents_arr_->data(), g_default_array_num, timeout);
	for (int i = 0; i < num; i++)
	{
		// 获取当前文件描述符和就绪的事件
		int fd = (*revents_arr_)[i].data.fd;
		uint32_t revents = (*revents_arr_)[i].events;
		if ((revents & EPOLLERR) || (revents & EPOLLHUP))
			revents = (EPOLLIN | EPOLLOUT);
		if ((revents & EPOLLIN) && checkFdIsInConnections(fd))
		{
			// 执行读方法
			connections_[fd]->recvData();
		}
		if ((revents & EPOLLOUT) && checkFdIsInConnections(fd))
		{
			// 执行写方法
			connections_[fd]->sendData();
		}
	}
}
```

### 实现`Listener`类基本结构

因为下层是通过`Connection`类对象来管理每一个链接，所以`Listener`类只需要作为`Connection`类的子类：

```cpp
class Listener : public Connection
{
};
```

接着，因为`Listener`类用于处理客户端和服务器端的链接，所以考虑在`Listener`类中添加一个成员表示TCP套接字，在构造函数初始化列表中进行初始化，如下：

!!! note

	这里使用到了[前面在「介绍HTTP协议基本结构与基本实现HTTPServer」封装的`TcpSocket`类](https://www.help-doc.top/Linux/26.%20%E5%BA%94%E7%94%A8%E5%B1%82%E5%8D%8F%E8%AE%AEHTTP/1.%20%E4%BB%8B%E7%BB%8DHTTP%E5%8D%8F%E8%AE%AE%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84%E4%B8%8E%E5%9F%BA%E6%9C%AC%E5%AE%9E%E7%8E%B0HTTPServer/1.%20%E4%BB%8B%E7%BB%8DHTTP%E5%8D%8F%E8%AE%AE%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84%E4%B8%8E%E5%9F%BA%E6%9C%AC%E5%AE%9E%E7%8E%B0HTTPServer.html#_6)

```cpp
class Listener : public Connection
{
public:
	Listener(uint16_t port = default_port)
		:bs_(std::make_shared<TcpSocket>(port))
	{
	}

private:
	std::shared_ptr<BaseSocket> bs_;
};
```

因为`Listener`类是`Connection`类的子类，所以需要重写父类的纯虚函数，但是目前不具体实现，如下：

=== "`recvData`"

	```cpp
	// 接收信息
	void recvData() override
	{}
	```

=== "`sendData`"

	```cpp
	// 发送信息
	void sendData() override
	{}
	```

=== "`handleException`"

	```cpp
	// 处理异常
	void handleException() override
	{}
	```

接着，在构造函数中进行初始化操作，包括创建套接字、绑定地址信息和设置监听操作。另外，因为每个`Connection`类对象都需要用到文件描述符，所以在构造函数中还需要将监听套接字设置到当前子类对象`Listener`中，便于使用`Connection`类对象可以获取到监听套接字，如下：

```cpp
Listener(uint16_t port = default_port)
	:bs_(std::make_shared<TcpSocket>(port))
{
	// 初始化套接字
	bs_->initSocket();
	// 调用父类函数设置监听套接字
	setFd(bs_->getListenSocketFd());
}
```

### 设计`Listener`类和`IOService`类

因为客户端和服务端建立连接本质就是服务端读取客户端的请求信息，所以`Listener`类只需要实现`recvData`函数即可，但是需要注意的是，本次实现的是边缘触发模式，虽然能确定`accept`函数一定不会阻塞，但是不能保证`accept`函数一定只读取一次，也就是说**需要将`accept`函数当做`read`等读取接口看待**，所以需要循环监听。另外，如果当做IO函数看待，那么必然会出现多读一次用于判断是否读取到结尾，但是这多读的一步会导致服务器阻塞，所以还需要将对应的监听套接字设置为非阻塞，所以首先需要将监听套接字设置为非阻塞

基于上面的思路，首先提供一个函数用于将文件描述符设置为非阻塞，因为这个函数在后续正常读取信息时也会用到，所以考虑将该函数放在一个工具类中：

```cpp
class EpollServerUtils
{
public:
	static void setNonBlock(int fd)
	{
		// 获取文件描述符已有的模式
		int mode = fcntl(fd, F_GETFL);
		if (mode < 0)
		{
			LOG(LogLevel::WARNING) << "获取文件描述符已有模式失败";
			exit(static_cast<int>(ErrorNumber::Fcntl_Fail));
		}

		// 设置非阻塞
		fcntl(fd, F_SETFL, mode | O_NONBLOCK);
	}
};
```

接着，在获取到`listen_socketfd`之后将其设置为非阻塞，在`BaseSocket`类中的`initSocket`函数中添加如下代码：

```cpp
class BaseSocket
{
public:
	// ...
	// 获取监听套接字
	virtual int getListenSocketFd() = 0;

	// 具体实现方法
	void initSocket()
	{
		// 创建套接字
        createSocket();
		// 设置非阻塞
		EpollServerUtils::setNonBlock(getListenSocketFd());
		// ...
	}
};
```

接着，为了拿到`accept`函数的错误码，可以考虑在`toAccept`函数的参数部分添加一个输出型参数`out_errno`：

=== "父类`BaseSocket`"

	```cpp
	class BaseSocket
	{
	public:
		// ...
		// 接收
		virtual int toAccept(SockAddrIn *client, int *out_errno) = 0;
		// ...
	};
	```

=== "子类`TcpSocket`"

	```cpp
	class TcpSocket : public BaseSocket
	{
	public:
		// ...
	
		// 实现接收
		int toAccept(SockAddrIn *client, int *out_errno) override
		{
			// ...
			int ac_socketfd = accept(_listen_socketfd, reinterpret_cast<struct sockaddr *>(&peer), &length);
			// 获取accept的错误码
			*out_errno = errno;
			// ...
		}
		// ...
	};
	```

基于上面的思路，实现`recvData`函数基本结构：

```cpp
// 接收信息
void recvData() override
{
	// 循环接收
	while(true)
	{
		int rerrno = 0;
		SockAddrIn client;
		int ac_socketfd = bs_->toAccept(std::addressof(client), &rerrno);
		// 当做IO行为处理
		if(ac_socketfd > 0)
		{
			// 正常情况，建立链接
		}
		else if(ac_socketfd < 0)
		{
			if(rerrno == EAGAIN || rerrno == EWOULDBLOCK)
			{
				LOG(LogLevel::INFO) << "accept接收结束";
				break;
			}
			else if(rerrno == EINTR)
			{
				LOG(LogLevel::INFO) << "accept被信号中断，重新接收";
				continue;
			}
			else
			{
				LOG(LogLevel::ERROR) << "accept错误";
				break;
			}
		}
	}
}
```

接着思考`recvData`函数的正常情况如何处理，在前面都是直接将指定的文件描述符添加到`epoll`模型中，但是在本次实现中，当前`Listener`类和`EpollServer`类之间存在一个`Connection`类，而`EpollServer`类只能看到`Connection`类对象，所以需要考虑将获取到的`ac_socketfd`封装为`Connection`类对象，再将该对象添加到`EpollServer`类中。但是，此处遇到两个问题：

1. 如何将获取到的`ac_socketfd`封装为`Connection`类对象
2. 如何将该对象添加到`EpollServer`类中

对于第一个问题，既然已经有了关于监听套接字的子类，那么自然还需要一个处理普通文件描述符的子类：

```cpp
class IOService : public Connection
{
public:
	void recvData()
	{
	}

	void sendData()
	{
	}

	void handleException()
	{
	}
};
```

对于第二个问题，这里需要在`Connection`类中添加一个成员变量指针，用于表示`EpollServer`类对象，这样在`Connection`类中就可以调用封装后的接口。这里就需要考虑如何在`Connection`类中访问到`EpollServer`类，又如何在`Connection`类中拿到`EpollServer`类对象

对于第一个问题，最直接的做法就是在`Connection`类所在文件中包含`EpollServer`类所在的文件，如下：

```cpp
// Connection类所在文件
#include "EpollServer.h"
```

但是这种做法有一个弊端，就是如果在`EpollServer`类所在的文件中包含了`Connection`类所在的文件，就会出现**头文件循环包含问题**，所以这种做法是不可取的。所以需要考虑使用前置声明的方式，如下：

```cpp
// Connection类所在文件
class EpollServer;
```

??? note "头文件循环包含问题"

	在`Connection`类所在的文件中需要用到`EpollServer`类，所以需要在`Connection`类所在的文件中添加`EpollServer`类的前置声明，但是注意，如果在`EpollServer`类所在文件中包含了`Connection`类所在文件，就**不要在`Connection`类所在的文件中再添加包含`EpollServer`类所在的文件**，防止出现头文件循环包含问题

但是，使用前置声明还会遇到一个问题：如果将前置声明放在`Connection`类所在的命名空间`connectionModule`中，会出现如下错误：

```cpp
// Connection类所在文件
class EpollServer;

namespace connectionModule
{
	class Connection
	{
	// ...
	protected:
		// 指向EpollServer指针
		std::weak_ptr<EpollServer> ep_svr_;
		// 报错："connectionModule::EpollServer" is ambiguous
	};
}
```

对于这种情况能想到的直接方案就是将前置声明放在命名空间`connectionModule`中，但是这样还会出现第二个问题：当前`EpollServer`的声明是在`Connection`类所在的命名空间`connectionModule`，而实际上`EpollServer`类的声明是在命名空间`epollServerModule`中，如果直接使用上面的方式：`std::weak_ptr<EpollServer> ep_svr_`，一旦在`Listener`中将该指针转换为`shared_ptr`类型，就会发现指针类型是`std::shared_ptr<connectionModule::EpollServer>`，而不是`std::shared_ptr<epollServerModule::EpollServer>`，而作为前置声明的`connectionModule::EpollServer`并没有完整的实现，这就导致访问不到`epollServerModule::EpollServer`中的成员。所以正确的做法是，将`EpollServer`类的前置声明放在`EpollServer`类所在的命名空间中，再将该前置声明整体放在`Connection`类所在文件的全局，并在使用到`EpollServer`的地方使用指定命名空间的方式使用`EpollServer`。为了防止出现<a href="javascript:;" class="custom-tooltip" data-title="见下面的note描述">循环引用问题</a>，需要使用`weak_ptr`而不再是`shared_ptr`，如下：

!!! note "上面提到的循环引用问题"

	`EpollServer`中管理了多个`Connection`对象指针，该指针是`shared_ptr`类型，如果再在`Connection`中使用`shared_ptr`就会出现`Connection`对象与`EpollServer`互指，一旦`Connection`类对象需要析构，就需要`EpollServer`先析构，而`EpollServer`要析构就需要`Connection`类对象析构导致循环引用问题

```cpp
// 前置声明
namespace epollServerModule
{
    class EpollServer;
}

namespace connectionModule
{
    class Connection
    {

    protected:
		// ...
        // 指向EpollServer指针
        std::weak_ptr<epollServerModule::EpollServer> ep_svr_;
		// ...
    };
}
```

但是，有这个指针还不够，还需要对这个指针进行初始化，为了防止忘记初始化该指针导致的空指针错误，考虑在构造函数中添加参数用于初始化该指针，如下：

```cpp
Connection(std::shared_ptr<epollServerModule::EpollServer> ep)
	: fd_(-1), events_(0), revents_(0), ep_svr_(ep) 
{
}
```

但是，`Connection`是`Listener`类的父类，所以在`Listener`类中的成员初始化之前需要先调用父类的构造函数初始化父类成员（除非父类构造函数是全缺省或者无参），所以需要在`Listener`类的构造函数的初始化列表同样添加该参数，如下：

```cpp
Listener(std::shared_ptr<epollServerModule::EpollServer> ep,  uint16_t port = default_port)
	: Connection(ep), bs_(std::make_shared<TcpSocket>(port))
{
	// 初始化套接字
	bs_->initSocket();
	// 调用父类函数设置监听套接字
	setFd(bs_->getListenSocketFd());
}
```

最后，不要遗忘还有一个子类`IOService`，同样，在其构造函数的初始化列表中同样添加该参数，如下：

```cpp
IOService(std::shared_ptr<epollServerModule::EpollServer> ep)
	: Connection(ep)
{}
```

接着，在`Connection`类中添加一个函数用于获取`EpollServer`类对象，如下：

```cpp
// 获取EpollServer指针
std::weak_ptr<epollServerModule::EpollServer> getEpollServer()
{
	return ep_svr_;
}
```

解决了上面的两个问题后，回到`Listener`类中的`recvData`函数编写剩下的逻辑。注意，要实现边缘触发模式一定要使用`EPOLLET`并且将对应的文件描述符设置为非阻塞：

```cpp
// 接收信息
void recvData() override
{
	// 循环接收
	while (true)
	{
		int rerrno = 0;
		SockAddrIn client;
		int ac_socketfd = bs_->toAccept(std::addressof(client), &rerrno);
		// 当做IO行为处理
		if (ac_socketfd > 0)
		{
			// 获取EpollServer类对象
			std::shared_ptr<epollServerModule::EpollServer> ep = getEpollServer().lock();
			// 正常情况，建立链接
			// 1. 创建连接对象
			std::shared_ptr<Connection> ac_con = std::make_shared<IOService>(ep);
			// 2. 设置非阻塞
			EpollServerUtils::setNonBlock(ac_socketfd);
			ac_con->setFd(ac_socketfd);
			ac_con->setEvent(EPOLLIN | EPOLLET);
			ac_con->setClient(std::move(client));
			// 3. 通过Connection将链接指针插入到Epoll模型中
			ep->insertFdAndEvents(ac_con);
		}
		// ...
	}
}
```

在上面的代码中，因为`getEpollServer`函数返回的是`weak_ptr`类型，所以需要调用`lock`函数将其转换为`shared_ptr`类型再使用。这里是临时提升为`shared_ptr`类型，所以使用完之后会自动释放，主要原因如下：`getEpollServer`持有的指针是对`EpollServer`类对象的弱引用，一旦`ep`变量离开作用域，哪怕`ep`变量是`shared_ptr`类型对`EpollServer`类对象的强引用，其引用计数器也不会等到`EpollServer`销毁再减1

最后，需要修改前面`TcpSocket`类中的`toAccept`函数，该函数中存在对`ac_socketfd < 0`的判断逻辑，但是这个逻辑已经在`loopOnce`函数处理了，所以需要删除`toAccept`函数中关于这部分的逻辑：

```cpp
// 实现接收
int toAccept(SockAddrIn *client, int *out_errno) override
{
	// ...
	// 获取accept的错误码
	*out_errno = errno;

	// 删除下面的逻辑——起始
	if (ac_socketfd < 0)
	{
	    LOG(LogLevel::ERROR) << "接收失败：" << strerror(errno);
	    exit(static_cast<int>(ErrorNumber::AcceptFail));
	}
	// 结束

	// ...
}
```

### 第一阶段测试

首先，在`IOService`类中的`recvData`函数中添加一条日志：

```cpp
void recvData()
{
	LOG(LogLevel::DEBUG) << "进入IOService读模块";
}
```

接着，创建一个主函数：

```cpp
#include "Listener.hpp"
#include "EpollServer.hpp"

using namespace listenerModule;
using namespace epollServerModule;

int main()
{
    std::shared_ptr<epollServerModule::EpollServer> e_svr = std::make_shared<epollServerModule::EpollServer>();
    // 创建Listener代表启动服务器
    // 将Listener套接字构建为Connection对象
    std::shared_ptr<Connection> con = std::make_shared<Listener>(e_svr);
    con->setFd(con->getFd());
	// 使用EPOLLET开启边缘触发模式
    con->setEvent(EPOLLIN | EPOLLET);
    // 将Connection对象插入到Epoll模型中
    e_svr->insertFdAndEvents(con);
    // 启动服务器
    e_svr->startServer();
    
    return 0;
}
```

编译运行上面的代码使用一个客户端连接就可以发现一个进程就可以处理多个连接，并且只要客户端向服务端发送内容就会打印类似下面的内容：

```
[2025-04-11 17-07-16] [DEBUG] [31041] [IOService.hpp] [20] - 进入IOService读模块
```

## Reactor模式（反应堆模式）

在上面的代码中已经实现了TCP服务器的下层框架，整体结构如下图所示：

<img src="33. epoll实现多路转接.assets/image-20250412161801832.png">

在上面的结构中，作为`Connection`类子类的`Listener`类一般被称为连接管理器，`IOService`类一般被称为IO处理器，而`EpollServer`类作为事件派发器，整个框架被称为Reactor模式

了解了何为Reactor模式后，下面就是继续完善上面的TCP服务器，因为上面的服务器还没有完成IO逻辑

## 完善TCP服务器的IO服务

### 数据准备

既然要做IO处理，那么少不了的就是定制协议，这样才可以尽可能保障客户端和服务端之间通信，本次考虑使用前面在[序列化和反序列化与网络计算器](https://www.help-doc.top/Linux/24.%20%E5%BA%8F%E5%88%97%E5%8C%96%E9%97%AE%E9%A2%98%E4%B8%8E%E7%BD%91%E7%BB%9C%E8%AE%A1%E7%AE%97%E5%99%A8/24.%20%E5%BA%8F%E5%88%97%E5%8C%96%E9%97%AE%E9%A2%98%E4%B8%8E%E7%BD%91%E7%BB%9C%E8%AE%A1%E7%AE%97%E5%99%A8.html#_1)封装的协议：`Response`类和`Request`类

### 完善`recvData`函数

接着完善`IOService`类中的`recvData`函数。考虑下面的思路：

既然是边缘触发模式，那么读取一定是持续读，所以需要死循环读取客户端发送的数据，得到结果后就需要对返回值进行判断，与前面`toAccept`函数的处理逻辑类似。对于读取异常的情况考虑将异常处理交给`handleException`函数，代码如下：

```cpp
void recvData()
{
	while (true)
	{
		char buffer[1024] = {0};
		ssize_t num = recv(getFd(), buffer, sizeof(buffer) - 1, 0);

		if(num > 0)
		{
			// 正常读取到数据
		}
		else if(num == 0)
		{
			// 读取到结尾
            handleException();
			break;
		}
		else 
		{
			if(errno == EAGAIN || errno == EWOULDBLOCK)
			{
				LOG(LogLevel::INFO) << "数据未准备完成，结束读取";
				break;
			}
			else if(errno == EINTR)
			{
				LOG(LogLevel::INFO) << "读取被信号中断，继续读取";
				continue;
			}
			else
			{
				// 读取出错
				handleException();
				return ;
			}
		}
	}
	
}
```

接着，考虑正常读取的情况，因为读取无法保证一次读取到所有的数据，所以需要循环读取，但是在上面的代码中，每一次循环的缓冲区`buffer`都是临时的，这就导致如果服务端还没有读完所有数据，那么上一次的数据就会被销毁，所以就需要用到`Connection`类中的`in_buffer_`保存当前读取到的数据，对此，在`Connection`类中提供一个设置`in_buffer_`的函数和获取`in_buffer_`的函数：

=== "设置`in_buffer_`"

	```cpp
	// 设置in_buffer_
	void setInBuffer(const std::string &in)
	{
		in_buffer_ += in;
	}
	```

=== "获取`in_buffer_`"

	```cpp
	// 获取in_buffer_
	std::string& getInBuffer()
	{
		return in_buffer_;
	}
	```

需要注意的是，对于获取函数来说，其返回值建议设置为引用版本，因为后面需要对`in_buffer_`进行直接修改

接着，在`recvData`函数的`num > 0`逻辑中通过`setInbuffer`将读取到的`buffer`存储到`in_buffer_`中，代码如下：

```cpp
void recvData()
{
	while (true)
	{
		char buffer[1024] = {0};
		ssize_t num = recv(getFd(), buffer, sizeof(buffer), 0);

		if(num > 0)
		{
			// 正常读取到数据
			setInbuffer(buffer);
		}
		// ...
	}
	
}
```

除了获取数据后还需要对数据进行处理，这里就需要开始使用协议，但是处理函数并不交给`IOService`类的`recvData`函数，而是考虑交给上层，首先在`IOService`类中添加一个函数对象成员，该函数对象的类型为`std::string(std::string&)`，代码如下：

```cpp
using handler_t = std::function<std::string(std::string&)>;

class IOService : public Connection
{
	// ...
private:
	handler_t handleData_;
};
```

接着，在`IOService`类中提供一个设置`handleData`函数对象的函数，代码如下：

```cpp
void setHandleDataFunc(handler_t func)
{
	handleData_ = func;
}
```

接着，考虑何时处理数据，在上面的`recvData`函数中，读取结束并且还在当前函数的情况说明这一次读取结束，所以在循环结束后就可以处理数据，即：

```cpp
void recvData()
{
	// ...

	// 处理数据
	std::string ret;
	if(handleData_)
		ret = handleData_(getInbuffer());
}
```

假设现在`handleData_`函数已经处理完数据并给出了有效的结果，接下来需要考虑的就是如何将结果发送给客户端，这个问题的本质就是写入，按照前面的逻辑就是让`EpollServer`关心当前文件描述符的写事件，但是实际上并不是如此，因为写事件看的只是当前进程的写入缓冲区，不看对方的缓冲区，对方缓冲区为满不发送这个行为交给TCP协议去做，而在最开始当前进程的写入缓冲区一定是空的，所以**写入事件一开始一定是就绪的**，但是存在一直写导致写入缓冲区写满的情况，所以此时再考虑让`EpollServer`关心当前文件描述符的写事件。综上所述，**写事件只有在写条件不满足时才进行关心**

接下来考虑如何让`EpollServer`关心当前文件描述符的写事件，按照前面的思路就是将当前文件描述符和写事件添加到`EpollServer`中，但是实际上这种方式只适用于常开启的事件，很明显，对于写事件来说并不是从一开始就需要关心，所以属于按需关心，那么就不能使用前面的思路。这里就需要考虑修改当前文件描述符对应的事件，所以需要在`EpollServer`类中提供一个修改当前文件描述符对应的事件的函数，而修改和添加只是使用的宏不同，所以可以考虑下面的设计：

=== "`epollCtrl`函数"

	```cpp
	void epollCtl(int fd, uint32_t events, int flag)
	{
		struct epoll_event ev;
		ev.events = events;
		ev.data.fd = fd;

		int ret = epoll_ctl(epfd_, flag, fd, &ev);
		// ...
	}
	```

=== "添加事件函数`addEvents`"

	```cpp
	void addEvents(int fd, uint32_t events)
	{
		epollCtl(fd, events, EPOLL_CTL_ADD);
	}
	```

=== "修改事件函数`modifyEvents`"

	```cpp
	void modifyEvents(int fd, uint32_t events)
	{
		epollCtl(fd, events, EPOLL_CTL_MOD);
	}
	```

接着，修改相关调用位置即可：

```cpp
// 添加文件描述符和关心的事件
void insertFdAndEvents(conn_t con)
{
	// ...

	// 添加到Epoll模型中
	ep_->addEvents(con->getFd(), con->getEvents());
}
```

接着，在`EpollServer`类中提供一个启动和取消写关心的函数：

```cpp
void EnbaleToWriteAndToRead(int fd, bool toRead, bool toWrite)
{
	uint32_t events = toRead ? EPOLLIN : 0 | toWrite ? EPOLLOUT : 0 | EPOLLET;
	// 1. 修改已有的哈希表节点
	auto pos = connections_.find(fd);
	if(pos != connections_.end())
	{
		connections_[fd]->setEvent(events);
		// 2. 写入到epoll模型中
		ep_->modifyEvents(fd, events);
	}
}
```

所以，在`recvData`函数中，只需要将`ret`中的结果存储到`out_buffer_`中再交给`sendData`函数处理即可。基于这个思路，首先需要提供一个设置`out_buffer_`的函数和一个获取`out_buffer_`的函数：

=== "设置`out_buffer_`"

	```cpp
	// 设置out_buffer_
	void setOutBuffer(const std::string &out)
	{
		out_buffer_ += out;
	}
	```

=== "获取`out_buffer_`"

	```cpp
	// 获取out_buffer_
	std::string& getOutBuffer()
	{
		return out_buffer_;
	}
	```

接着完善`recvData`函数，代码如下：

```cpp
void recvData()
{
	// ...

	// 添加结果到结果字符串中
	setOutBuffer(ret);

	if(!getOutBuffer().empty())
	{
		std::shared_ptr<epollServerModule::EpollServer> e_svr = getEpollServer().lock();
		e_svr->EnbaleToWriteAndToRead(getFd(), true, true);
	}
}
```

### 完善上层处理函数

创建一个类表示任务，其中包含一个处理静态函数：

```cpp
class Task
{
public:
	static std::string task_1(std::string& in)
	{

	}
};
```

接下来考虑的就是如何设计这个任务函数。可以考虑思路：对得到的字符串进行解码+反序列化得到有效载荷，然后根据有效载荷进行计算，最后将结果序列化+编码并返回。这里需要用到前面在[序列化和反序列化与网络计算器](https://www.help-doc.top/Linux/24.%20%E5%BA%8F%E5%88%97%E5%8C%96%E9%97%AE%E9%A2%98%E4%B8%8E%E7%BD%91%E7%BB%9C%E8%AE%A1%E7%AE%97%E5%99%A8/24.%20%E5%BA%8F%E5%88%97%E5%8C%96%E9%97%AE%E9%A2%98%E4%B8%8E%E7%BD%91%E7%BB%9C%E8%AE%A1%E7%AE%97%E5%99%A8.html#_12)设计的`Caculator`类。参考代码如下：

```cpp
static std::string task_1(std::string& in)
{
	std::string json_str;
	std::string result;
	// 解码
	while(decode(in, json_str))
	{
		std::string resp_str;
		if(json_str.empty())
		{
			LOG(LogLevel::WARNING) << "解码失败";
			break;
		}

		// 反序列化
		Request req;
		if(!req.deserialize(json_str))
		{
			LOG(LogLevel::WARNING) << "反序列化失败";
			break;
		}

		Calculator c;
		Response resp = c.calculate(req);

		// 序列化
		if (!resp.serialize(resp_str))
		{
			LOG(LogLevel::WARNING) << "序列化失败";
			break;
		}

		// 编码
		if (!encode(resp_str))
		{
			LOG(LogLevel::WARNING) << "编码失败";
			break;
		}

		result += resp_str;
	}

	return result;
}
```

接着，在调用`IOService`类中的`recvData`函数之前先设置`handleData`，本次考虑在`Listener`类中的`recvData`函数中进行设置：

```cpp
// 接收信息
void recvData() override
{
	// 循环接收
	while (true)
	{
		int rerrno = 0;
		SockAddrIn client;
		int ac_socketfd = bs_->toAccept(std::addressof(client), &rerrno);
		// 当做IO行为处理
		if (ac_socketfd > 0)
		{
			// ...
			// 4. 设置IOService类中的方法
			std::shared_ptr<IOService> ptr = std::dynamic_pointer_cast<IOService>(ac_con);
			ptr->setHandleDataFunc(taskModule::Task::task_1);
		}
		// ...
	}
}
```

### 完善`sendData`函数

`sendData`函数与`recvData`函数的逻辑基本类似，下面给出基本结构：

```cpp
void sendData()
{
	while(true)
	{
		ssize_t num = send(getFd(), getOutBuffer().c_str(), getOutBuffer().size(), 0);
		if(num > 0)
		{
			// 正常发送
		}
		else if(num == 0)
		{
			// 发送完毕
			break;
		}
		else
		{
			if(errno == EAGAIN || errno == EWOULDBLOCK)
			{
				LOG(LogLevel::INFO) << "缓冲区已满";
				break;
			}
			else if(errno == EINTR)
			{
				LOG(LogLevel::INFO) << "信号中断";
				continue;
			}
			else 
			{
				handleException();
				return ;
			}
		}
	}
}
```

接下来考虑正常发送的情况，实际上，既然是正常发送，那么说明`num`个数据已经被发送出去了，所以需要将`out_buffer_`中的数据删除，所以需要在`Connection`类中提供一个删除`out_buffer_`中数据的函数，代码如下：

```cpp
// 移除n个字符
void remove_n_data(int n)
{
	out_buffer_.erase(0, n);
}
```

完善`sendData`函数：

```cpp
void sendData()
{
	while(true)
	{
		ssize_t num = send(getFd(), getOutBuffer().c_str(), getOutBuffer().size(), 0);
		if(num > 0)
		{
			// 正常发送
			remove_n_data(num);
		}
		// ...
	}
}
```

完成发送之后还需要处理一件事情：将当前文件描述符从`EpollServer`中移除，因为当前文件描述符对应的写事件已经处理完毕，所以不再需要关心当前文件描述符的写事件，下一次肯定也有空间写入，如果不存在空间再重新关心。但是上面的实现中有两种离开循环的情况：

1. 发送缓冲区已满
2. 数据发送完毕

对于第一种情况，既然是发送完毕，那么`out_buffer_`一定为空，所以可以直接将当前文件描述符从`EpollServer`中移除，对于第二种情况，依旧需要保证`EpollServer`对当前文件描述符写事件的关心：

```cpp
void sendData()
{
	// ...

	if(getOutBuffer().empty()) // 发完数据
	{
		std::shared_ptr<epollServerModule::EpollServer> e_svr = getEpollServer().lock();
		e_svr->EnbaleToWriteAndToRead(getFd(), true, false);
	}
	else 
	{
		std::shared_ptr<epollServerModule::EpollServer> e_svr = getEpollServer().lock();
		e_svr->EnbaleToWriteAndToRead(getFd(), true, true);
	}
}
```

!!! note "为什么`recvData`函数中开启写事件关心了还要在`sendData`函数中再关心一次？"

	在`recvData`函数中开启写事件关心是因为确保下一次可以进入到`loopOnce`函数的写逻辑中执行`sendData`函数，而`sendData`函数中再开启写事件关心是因为可能存在写缓冲区满的情况导致无法正常写入，此时需要对写事件关心从而保证下一次可以正常写入

	在`recvData`函数中，也可以不开启写事件关心直接调用`sendData`函数，效果都是一样的

### 完善`handleException`函数

在前面`EpollServer`类中，将`EPOLLERR`和`EPOLLHUP`全部设置为`EPOLLIN | EPOLLOUT`就是为了统一处理异常，这样，不论是在`recvData`出异常还是在`sendData`出异常，都会因为关心的事件进入对应的函数从而触发对应函数中调用`handleException`函数的逻辑。下面完善`handleException`函数，考虑思路：既然是处理异常，那么说明当前文件描述符出现了问题，此时要做的就是释放资源，所以按照下面三个步骤设计`handleException`函数：

1. 从`Epoll`中移除当前文件描述符
2. 关闭当前文件描述符
3. 从哈希表中移除当前文件描述符

根据这三个步骤，在`Epoll`中分别提供移除接口：

```cpp
// 移除文件描述符和事件
void deleteEvents(int fd)
{
	epollCtl(fd, 0, EPOLL_CTL_DEL);
}
```

接着，在`Connection`类中提供关闭文件描述符：

```cpp
// 关闭指定的文件描述符
void closeFd()
{
	close(fd_);
}
```

最后，在`EpollServer`类中提供从哈希表中移除当前文件描述符：

```cpp
// 从哈希表中移除当前文件描述符的函数
void popFdAndEventsFromConnections(int fd)
{
	if(connections_.find(fd) != connections_.end())
	{
		connections_.erase(fd);
	}
}
```

但是，这三个步骤实际上只能在`EpollServer`类中进行，所以可以考虑在调用`popFdAndEventsFromConnections`时执行这三个步骤：

```cpp
// 从哈希表中移除当前文件描述符的函数
void popFdAndEventsFromConnections(int fd)
{
	if(connections_.find(fd) != connections_.end())
	{
		// 1. 移除事件
		ep_->deleteEvents(fd);
		// 2. 关闭文件描述符
		connections_[fd]->closeFd();
		// 3. 删除文件描述符
		connections_.erase(fd);
	}
}
```

最后，在`handleException`函数中调用即可：

```cpp
void handleException()
{
    setHandleDataFunc(nullptr);
	std::shared_ptr<epollServerModule::EpollServer> e_svr = getEpollServer().lock();
	e_svr->popFdAndEventsFromConnections();
}
```

### 第二阶段测试

本次使用前面在[序列化和反序列化与网络计算器](https://www.help-doc.top/Linux/24.%20%E5%BA%8F%E5%88%97%E5%8C%96%E9%97%AE%E9%A2%98%E4%B8%8E%E7%BD%91%E7%BB%9C%E8%AE%A1%E7%AE%97%E5%99%A8/24.%20%E5%BA%8F%E5%88%97%E5%8C%96%E9%97%AE%E9%A2%98%E4%B8%8E%E7%BD%91%E7%BB%9C%E8%AE%A1%E7%AE%97%E5%99%A8.html#_15)设计的主函数测试

正常情况下不论多少个客户端连接都可以正常计算，但是一旦有个客户端断开，那么其他客户端此时再发送请求就会出现问题，控制台输出如下：

```
...
[2025-04-13 12-57-52] [DEBUG] [21818] [Task.hpp] [16] - 进入任务处理
malloc(): unaligned tcache chunk detected
已中止 (核心已转储)
```

从上面的日志可以发现，错误出现在进入任务处理函数时，所以可以基本确定是调用了任务函数时出现的问题，结合触发条件：客户端断开连接，可以判断出错误可能是由`handleException`引起的，根据这两个推测，问题基本可以定位在下面的位置：

=== "`Listener`类的`recvData`函数"

	```cpp
	// 接收信息
	void recvData() override
	{
		// 循环接收
		while (true)
		{
			int rerrno = 0;
			SockAddrIn client;
			int ac_socketfd = bs_->toAccept(std::addressof(client), &rerrno);
			// 当做IO行为处理
			if (ac_socketfd > 0)
			{
				// LOG(LogLevel::DEBUG) << "准备IO套接字";
				// 获取EpollServer类对象
				std::shared_ptr<epollServerModule::EpollServer> ep = getEpollServer().lock();
				// 正常情况，建立链接
				// 1. 创建连接对象
				std::shared_ptr<Connection> ac_con = std::make_shared<IOService>(ep);
				// ...
				// 4. 设置IOService类中的方法
				std::shared_ptr<IOService> ptr = std::dynamic_pointer_cast<IOService>(ac_con);
				ptr->setHandleDataFunc(taskModule::Task::task_1);
			}
			// ...
		}
	}
	```

=== "`IOService`类中的`recvData`函数"

	```cpp
	void recvData()
	{
		//...	
		// 处理数据
		std::string ret;
		if (handleData_)
			ret = handleData_(getInBuffer());

		// 添加结果到结果字符串中
		setOutBuffer(ret);

		if(!getOutBuffer().empty())
		{
			LOG(LogLevel::DEBUG) << "recvData开启写事件关心";
			std::shared_ptr<epollServerModule::EpollServer> e_svr = getEpollServer().lock();
			e_svr->EnbaleToWriteAndToRead(getFd(), true, true);
		}
	}
	```

=== "任务函数`task_1`"

	```cpp
	static std::string task_1(std::string &in)
	{
		LOG(LogLevel::DEBUG) << "进入任务处理";
		std::string json_str;
		std::string result;
		// 解码
		while (decode(in, json_str))
		{
			std::string resp_str;
			if (json_str.empty())
			{
				LOG(LogLevel::WARNING) << "解码失败";
				break;
			}

			// 反序列化
			Request req;
			if (!req.deserialize(json_str))
			{
				LOG(LogLevel::WARNING) << "反序列化失败";
				break;
			}

			Calculator c;
			Response resp = c.calculate(req);

			// 序列化
			if (!resp.serialize(resp_str))
			{
				LOG(LogLevel::WARNING) << "序列化失败";
				break;
			}

			// 编码
			if (!encode(resp_str))
			{
				LOG(LogLevel::WARNING) << "编码失败";
				break;
			}

			result += resp_str;
		}

		LOG(LogLevel::DEBUG) << "任务处理完成";
		LOG(LogLevel::DEBUG) << result;

		return result;
	}
	```

但是，具体是什么问题还需要进一步分析。而`malloc(): unaligned tcache chunk detected`一般可能是下面几种原因：

1. 缓冲区溢出：写入超过分配的内存边界
2. 野指针操作：访问已释放的内存区域
3. 错误的指针运算：如对指针进行不正确的类型转换或算术运算
4. 多线程竞争：未加锁的并发内存操作

而在本次实现中，只有第一种情况和第二种情况，所以需要进一步分析，这里可以使用AddressSanitizer（ASan）内存检测工具，这个工具可以检测以下问题：

1. 缓冲区溢出（堆/栈/全局变量）
2. 使用释放后的内存（use-after-free）
3. 双重释放（double-free）
4. 内存泄漏（memory leaks）

在编译时添加`-fsanitize=address`即可开启ASan检测，再带上`-g`可以看到更多的信息，例如符号表、源代码行号等，例如本次编译指令为：

```bash
g++ -o main main.cc -lpthread -ljsoncpp -fsanitize=address -g
```

重新编译上面的代码，运行程序并开启一个客户端让其断开，可以看到控制台有很长的输出

??? abstract "ASan输出"

```
[2025-04-13 13-12-59] [DEBUG] [22204] [IOService.hpp] [141] - 进入设置上层处理函数
=================================================================
==22204==ERROR: AddressSanitizer: heap-use-after-free on address 0x511000000100 at pc 0x5934f277824f bp 0x7ffd85516c30 sp 0x7ffd85516c20
READ of size 8 at 0x511000000100 thread T0

    #0 0x5934f277824e in std::_Function_base::_M_empty() const /usr/include/c++/13/bits/std_function.h:247
    #1 0x5934f278a59f in std::function<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > (std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >&)>::operator bool() const /usr/include/c++/13/bits/std_function.h:574
    #2 0x5934f2782868 in IOServiceModule::IOService::recvData() /home/epsda/Codes_In_Linux/EpollServer_ET/IOService.hpp:66
    #3 0x5934f277f51c in epollServerModule::EpollServer::loopOnce() /home/epsda/Codes_In_Linux/EpollServer_ET/EpollServer.hpp:78
    #4 0x5934f277f76e in epollServerModule::EpollServer::startServer() /home/epsda/Codes_In_Linux/EpollServer_ET/EpollServer.hpp:97
    #5 0x5934f2777531 in main /home/epsda/Codes_In_Linux/EpollServer_ET/main.cc:21
    #6 0x7c6e95a2a1c9 in __libc_start_call_main ../sysdeps/nptl/libc_start_call_main.h:58
    #7 0x7c6e95a2a28a in __libc_start_main_impl ../csu/libc-start.c:360
    #8 0x5934f2775f74 in _start (/home/epsda/Codes_In_Linux/EpollServer_ET/main+0x6f74) (BuildId: f7d36700d4132470fb997f44c7b71a254a08564e)

0x511000000100 is located 192 bytes inside of 208-byte region [0x511000000040,0x511000000110)
freed by thread T0 here:
    #0 0x7c6e962ff5e8 in operator delete(void*, unsigned long) ../../../../src/libsanitizer/asan/asan_new_delete.cpp:164
    #1 0x5934f2797ee3 in std::__new_allocator<std::_Sp_counted_ptr_inplace<IOServiceModule::IOService, std::allocator<void>, (__gnu_cxx::_Lock_policy)2> >::deallocate(std::_Sp_counted_ptr_inplace<IOServiceModule::IOService, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>*, unsigned long) /usr/include/c++/13/bits/new_allocator.h:172
    #2 0x5934f27959f8 in std::allocator_traits<std::allocator<std::_Sp_counted_ptr_inplace<IOServiceModule::IOService, std::allocator<void>, (__gnu_cxx::_Lock_policy)2> > >::deallocate(std::allocator<std::_Sp_counted_ptr_inplace<IOServiceModule::IOService, std::allocator<void>, (__gnu_cxx::_Lock_policy)2> >&, std::_Sp_counted_ptr_inplace<IOServiceModule::IOService, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>*, unsigned long) /usr/include/c++/13/bits/alloc_traits.h:517
    #3 0x5934f27959f8 in std::__allocated_ptr<std::allocator<std::_Sp_counted_ptr_inplace<IOServiceModule::IOService, std::allocator<void>, (__gnu_cxx::_Lock_policy)2> > >::~__allocated_ptr() /usr/include/c++/13/bits/allocated_ptr.h:74
    #4 0x5934f2798f4a in std::_Sp_counted_ptr_inplace<IOServiceModule::IOService, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_destroy() /usr/include/c++/13/bits/shared_ptr_base.h:623
    #5 0x5934f277804d in std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release() /usr/include/c++/13/bits/shared_ptr_base.h:347
    #6 0x5934f2786ce7 in std::__shared_count<(__gnu_cxx::_Lock_policy)2>::~__shared_count() /usr/include/c++/13/bits/shared_ptr_base.h:1071
    #7 0x5934f277e6e7 in std::__shared_ptr<connectionModule::Connection, (__gnu_cxx::_Lock_policy)2>::~__shared_ptr() /usr/include/c++/13/bits/shared_ptr_base.h:1524
    #8 0x5934f277e707 in std::shared_ptr<connectionModule::Connection>::~shared_ptr() /usr/include/c++/13/bits/shared_ptr.h:175
    #9 0x5934f277e72b in std::pair<int const, std::shared_ptr<connectionModule::Connection> >::~pair() /usr/include/c++/13/bits/stl_pair.h:187
    #10 0x5934f2791e02 in void std::__new_allocator<std::__detail::_Hash_node<std::pair<int const, std::shared_ptr<connectionModule::Connection> >, false> >::destroy<std::pair<int const, std::shared_ptr<connectionModule::Connection> > >(std::pair<int const, std::shared_ptr<connectionModule::Connection> >*) /usr/include/c++/13/bits/new_allocator.h:198
    #11 0x5934f2791e02 in void std::allocator_traits<std::allocator<std::__detail::_Hash_node<std::pair<int const, std::shared_ptr<connectionModule::Connection> >, false> > >::destroy<std::pair<int const, std::shared_ptr<connectionModule::Connection> > >(std::allocator<std::__detail::_Hash_node<std::pair<int const, std::shared_ptr<connectionModule::Connection> >, false> >&, std::pair<int const, std::shared_ptr<connectionModule::Connection> >*) /usr/include/c++/13/bits/alloc_traits.h:558
    #12 0x5934f2791e02 in std::__detail::_Hashtable_alloc<std::allocator<std::__detail::_Hash_node<std::pair<int const, std::shared_ptr<connectionModule::Connection> >, false> > >::_M_deallocate_node(std::__detail::_Hash_node<std::pair<int const, std::shared_ptr<connectionModule::Connection> >, false>*) /usr/include/c++/13/bits/hashtable_policy.h:2011
    #13 0x5934f2793244 in std::_Hashtable<int, std::pair<int const, std::shared_ptr<connectionModule::Connection> >, std::allocator<std::pair<int const, std::shared_ptr<connectionModule::Connection> > >, std::__detail::_Select1st, std::equal_to<int>, std::hash<int>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<false, false, true> >::_M_erase(unsigned long, std::__detail::_Hash_node_base*, std::__detail::_Hash_node<std::pair<int const, std::shared_ptr<connectionModule::Connection> >, false>*) /usr/include/c++/13/bits/hashtable.h:2353
    #14 0x5934f279027c in std::_Hashtable<int, std::pair<int const, std::shared_ptr<connectionModule::Connection> >, std::allocator<std::pair<int const, std::shared_ptr<connectionModule::Connection> > >, std::__detail::_Select1st, std::equal_to<int>, std::hash<int>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<false, false, true> >::_M_erase(std::integral_constant<bool, true>, int const&) /usr/include/c++/13/bits/hashtable.h:2396
    #15 0x5934f278d2c2 in std::_Hashtable<int, std::pair<int const, std::shared_ptr<connectionModule::Connection> >, std::allocator<std::pair<int const, std::shared_ptr<connectionModule::Connection> > >, std::__detail::_Select1st, std::equal_to<int>, std::hash<int>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<false, false, true> >::erase(int const&) /usr/include/c++/13/bits/hashtable.h:984
    #16 0x5934f278a0d2 in std::unordered_map<int, std::shared_ptr<connectionModule::Connection>, std::hash<int>, std::equal_to<int>, std::allocator<std::pair<int const, std::shared_ptr<connectionModule::Connection> > > >::erase(int const&) /usr/include/c++/13/bits/unordered_map.h:770
    #17 0x5934f277fc47 in epollServerModule::EpollServer::popFdAndEventsFromConnections(int) /home/epsda/Codes_In_Linux/EpollServer_ET/EpollServer.hpp:132
    #18 0x5934f27842b7 in IOServiceModule::IOService::handleException() /home/epsda/Codes_In_Linux/EpollServer_ET/IOService.hpp:136
    #19 0x5934f2782315 in IOServiceModule::IOService::recvData() /home/epsda/Codes_In_Linux/EpollServer_ET/IOService.hpp:40
    #20 0x5934f277f51c in epollServerModule::EpollServer::loopOnce() /home/epsda/Codes_In_Linux/EpollServer_ET/EpollServer.hpp:78
    #21 0x5934f277f76e in epollServerModule::EpollServer::startServer() /home/epsda/Codes_In_Linux/EpollServer_ET/EpollServer.hpp:97
    #22 0x5934f2777531 in main /home/epsda/Codes_In_Linux/EpollServer_ET/main.cc:21
    #23 0x7c6e95a2a1c9 in __libc_start_call_main ../sysdeps/nptl/libc_start_call_main.h:58
    #24 0x7c6e95a2a28a in __libc_start_main_impl ../csu/libc-start.c:360
    #25 0x5934f2775f74 in _start (/home/epsda/Codes_In_Linux/EpollServer_ET/main+0x6f74) (BuildId: f7d36700d4132470fb997f44c7b71a254a08564e)

previously allocated by thread T0 here:
    #0 0x7c6e962fe548 in operator new(unsigned long) ../../../../src/libsanitizer/asan/asan_new_delete.cpp:95
    #1 0x5934f2797e8b in std::__new_allocator<std::_Sp_counted_ptr_inplace<IOServiceModule::IOService, std::allocator<void>, (__gnu_cxx::_Lock_policy)2> >::allocate(unsigned long, void const*) /usr/include/c++/13/bits/new_allocator.h:151
    #2 0x5934f2795941 in std::allocator_traits<std::allocator<std::_Sp_counted_ptr_inplace<IOServiceModule::IOService, std::allocator<void>, (__gnu_cxx::_Lock_policy)2> > >::allocate(std::allocator<std::_Sp_counted_ptr_inplace<IOServiceModule::IOService, std::allocator<void>, (__gnu_cxx::_Lock_policy)2> >&, unsigned long) /usr/include/c++/13/bits/alloc_traits.h:482
    #3 0x5934f2795941 in std::__allocated_ptr<std::allocator<std::_Sp_counted_ptr_inplace<IOServiceModule::IOService, std::allocator<void>, (__gnu_cxx::_Lock_policy)2> > > std::__allocate_guarded<std::allocator<std::_Sp_counted_ptr_inplace<IOServiceModule::IOService, std::allocator<void>, (__gnu_cxx::_Lock_policy)2> > >(std::allocator<std::_Sp_counted_ptr_inplace<IOServiceModule::IOService, std::allocator<void>, (__gnu_cxx::_Lock_policy)2> >&) /usr/include/c++/13/bits/allocated_ptr.h:98
    #4 0x5934f27938ae in std::__shared_count<(__gnu_cxx::_Lock_policy)2>::__shared_count<IOServiceModule::IOService, std::allocator<void>, std::shared_ptr<epollServerModule::EpollServer>&>(IOServiceModule::IOService*&, std::_Sp_alloc_shared_tag<std::allocator<void> >, std::shared_ptr<epollServerModule::EpollServer>&) /usr/include/c++/13/bits/shared_ptr_base.h:969
    #5 0x5934f2790c79 in std::__shared_ptr<IOServiceModule::IOService, (__gnu_cxx::_Lock_policy)2>::__shared_ptr<std::allocator<void>, std::shared_ptr<epollServerModule::EpollServer>&>(std::_Sp_alloc_shared_tag<std::allocator<void> >, std::shared_ptr<epollServerModule::EpollServer>&) /usr/include/c++/13/bits/shared_ptr_base.h:1712
    #6 0x5934f278d9be in std::shared_ptr<IOServiceModule::IOService>::shared_ptr<std::allocator<void>, std::shared_ptr<epollServerModule::EpollServer>&>(std::_Sp_alloc_shared_tag<std::allocator<void> >, std::shared_ptr<epollServerModule::EpollServer>&) /usr/include/c++/13/bits/shared_ptr.h:464
    #7 0x5934f278ada5 in std::shared_ptr<std::enable_if<!std::is_array<IOServiceModule::IOService>::value, IOServiceModule::IOService>::type> std::make_shared<IOServiceModule::IOService, std::shared_ptr<epollServerModule::EpollServer>&>(std::shared_ptr<epollServerModule::EpollServer>&) /usr/include/c++/13/bits/shared_ptr.h:1010
    #8 0x5934f2784f65 in listenerModule::Listener::recvData() /home/epsda/Codes_In_Linux/EpollServer_ET/Listener.hpp:49
    #9 0x5934f277f51c in epollServerModule::EpollServer::loopOnce() /home/epsda/Codes_In_Linux/EpollServer_ET/EpollServer.hpp:78
    #10 0x5934f277f76e in epollServerModule::EpollServer::startServer() /home/epsda/Codes_In_Linux/EpollServer_ET/EpollServer.hpp:97
    #11 0x5934f2777531 in main /home/epsda/Codes_In_Linux/EpollServer_ET/main.cc:21
    #12 0x7c6e95a2a1c9 in __libc_start_call_main ../sysdeps/nptl/libc_start_call_main.h:58
    #13 0x7c6e95a2a28a in __libc_start_main_impl ../csu/libc-start.c:360
    #14 0x5934f2775f74 in _start (/home/epsda/Codes_In_Linux/EpollServer_ET/main+0x6f74) (BuildId: f7d36700d4132470fb997f44c7b71a254a08564e)

SUMMARY: AddressSanitizer: heap-use-after-free /usr/include/c++/13/bits/std_function.h:247 in std::_Function_base::_M_empty() const
Shadow bytes around the buggy address:
  0x510ffffffe80: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x510fffffff00: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x510fffffff80: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x511000000000: fa fa fa fa fa fa fa fa fd fd fd fd fd fd fd fd
  0x511000000080: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd
=>0x511000000100:[fd]fd fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x511000000180: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x511000000200: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x511000000280: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x511000000300: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x511000000380: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
Shadow byte legend (one shadow byte represents 8 application bytes):
  Addressable:           00
  Partially addressable: 01 02 03 04 05 06 07 
  Heap left redzone:       fa
  Freed heap region:       fd
  Stack left redzone:      f1
  Stack mid redzone:       f2
  Stack right redzone:     f3
  Stack after return:      f5
  Stack use after scope:   f8
  Global redzone:          f9
  Global init order:       f6
  Poisoned by user:        f7
  Container overflow:      fc
  Array cookie:            ac
  Intra object redzone:    bb
  ASan internal:           fe
  Left alloca redzone:     ca
  Right alloca redzone:    cb
==22204==ABORTING
```

根据上面的错误信息，首先看问题原因，即：`AddressSanitizer: heap-use-after-free on address`，可以得出结论：出现了野指针问题。接着看后面的内容可以得出问题出现在`handleData_`的位置出现了野指针问题

接着，再看第二部分`freed by thread T0 here`，这一行提示了指针是在何处进行的释放：

从当前实现代码的堆栈信息结合当前部分的前面堆栈信息可以发现问题出现在`connections_`移除元素的位置，尤其是出现了大量的智能指针

最后看`previously allocated by thread T0 here`，表示上一次开辟空间的位置，可以看到出现在`Connection`的`recvData`函数中，所以现在问题就可以定位在调用`recvData`的位置以及`recvData`函数中，因为这两个位置涉及到了`Connection`的创建和销毁

回到代码，下面看调用位置和两个子类的`recvData`函数：

=== "调用位置"

	```cpp
	// 单次循环
	void loopOnce()
	{
		// ...
		for (int i = 0; i < num; i++)
		{
			// ...
			if ((revents & EPOLLIN) && checkFdIsInConnections(fd))
			{
				// 执行读方法
				connections_[fd]->recvData();
			}
			// ...
		}
	}
	```

=== "子类`Listener`的`recvData`函数"

	```cpp
	// 接收信息
	void recvData() override
	{
		// 循环接收
		while (true)
		{
			int rerrno = 0;
			SockAddrIn client;
			int ac_socketfd = bs_->toAccept(std::addressof(client), &rerrno);
			// 当做IO行为处理
			if (ac_socketfd > 0)
			{
				// LOG(LogLevel::DEBUG) << "准备IO套接字";
				// 获取EpollServer类对象
				std::shared_ptr<epollServerModule::EpollServer> ep = getEpollServer().lock();
				// 正常情况，建立链接
				// 1. 创建连接对象
				std::shared_ptr<Connection> ac_con = std::make_shared<IOService>(ep);
				// IOService *ac_con = new IOService(ep);
				// 2. 设置非阻塞
				EpollServerUtils::setNonBlock(ac_socketfd);
				ac_con->setFd(ac_socketfd);
				ac_con->setEvent(EPOLLIN | EPOLLET);
				ac_con->setClient(std::move(client));
				// LOG(LogLevel::DEBUG) << "开始添加Connection链接到Epoll模型中";
				// 3. 通过Connection将链接指针插入到Epoll模型中
				ep->insertFdAndEvents(ac_con);
				// LOG(LogLevel::DEBUG) << "IO套接字准备完成";
				// 4. 设置IOService类中的方法
				std::shared_ptr<IOService> ptr = std::dynamic_pointer_cast<IOService>(ac_con);
				// LOG(LogLevel::DEBUG) << "Listener模块中IOService对象的引用计数值：" << ac_con.use_count();
				// ptr->setHandleDataFunc(taskModule::Task::task_1);
				ptr->setHandleDataFunc(taskModule::Task::task_1);
			}
			// ...
		}
	}
	```

=== "子类`IOService`的`recvData`函数"

	```cpp
	void recvData()
	{
		while (true)
		{
			char buffer[1024] = {0};
			ssize_t num = recv(getFd(), buffer, sizeof(buffer) - 1, 0);

			if(num > 0)
			{
				// 正常读取到数据
				setInBuffer(buffer);
			}
			else if(num == 0)
			{
				// 读取到结尾，对端关闭
				handleException();
				break;
			}
			else 
			{
				if(errno == EAGAIN || errno == EWOULDBLOCK)
				{
					LOG(LogLevel::INFO) << "数据未准备完成，结束读取";
					break;
				}
				else if(errno == EINTR)
				{
					LOG(LogLevel::INFO) << "读取被信号中断，继续读取";
					continue;
				}
				else
				{
					// 读取出错
					handleException();
					return ;
				}
			}
		}
		
		// 处理数据
		std::string ret;
		if (handleData_)
			ret = handleData_(getInBuffer());

		// 添加结果到结果字符串中
		setOutBuffer(ret);

		if(!getOutBuffer().empty())
		{
			LOG(LogLevel::DEBUG) << "recvData开启写事件关心";
			std::shared_ptr<epollServerModule::EpollServer> e_svr = getEpollServer().lock();
			e_svr->EnbaleToWriteAndToRead(getFd(), true, true);
		}
	}
	```

结合触发问题的原因：客户端断开连接，可以定位到`IOService`的`recvData`函数的`handleException`，该函数会对相关的资源进行释放，其中包括从哈希表中移除指定的文件描述符，但是，移除完毕后，`recvData`函数还会继续向下执行，此时`handleData_`是`nullptr`，但是此时的`this`所代表的智能指针已经因为引用计数变为0而被释放，所以调用`setOutBuffer`函数时就会出现野指针问题

解决这个问题的方式就是**确保智能指针在执行完`recvData`之前还有效**，这就需要回到`loopOnce`函数中，当前是直接通过`key`从哈希表中取出智能指针的方式访问`recvData`，而其中的`handleException`函数一旦从哈希表中移除指定的键值对，那么对应的智能指针就被释放，所以为了防止智能指针被提前释放，可以考虑暂时提升指定智能指针的引用计数，再执行完`recvData`后再自动销毁：

```cpp
 // 单次循环
void loopOnce()
{
	// ...
	for (int i = 0; i < num; i++)
	{
		// ...
		if ((revents & EPOLLIN) && checkFdIsInConnections(fd))
		{
			// 执行读方法
			// 提升引用计数，确保recvData结束再销毁智能指针
			auto ptr = connections_[fd];
			ptr->recvData();
		}
		// ...
	}
}
```

同样，对于`sendData`函数也可能存在对应的问题，所以也需要进行类似的处理：

```cpp
 // 单次循环
void loopOnce()
{
	// ..
	for (int i = 0; i < num; i++)
	{
		// ...
		if ((revents & EPOLLOUT) && checkFdIsInConnections(fd))
		{
			// 执行写方法
			// 提升引用计数，确保sendData结束再销毁智能指针
			auto ptr = connections_[fd];
			ptr->sendData();
			// connections_[fd]->sendData();
		}
	}
}
```

再次测试运行即可发现没有出现野指针问题，并且客户端也可以正常断开连接